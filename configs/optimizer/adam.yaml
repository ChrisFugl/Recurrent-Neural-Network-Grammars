optimizer:
  type: 'adam'
  learning_rate: 1e-3
  beta1: 0.9
  beta2: 0.999
  epsilon: 1e-8
  weight_decay: 0
  amsgrad: false
